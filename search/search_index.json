{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome","text":""},{"location":"#musing","title":"mus\u00b7ing","text":"<p>n.</p> <ol> <li>Contemplation; meditation.  </li> <li>A product of contemplation; a thought</li> </ol>"},{"location":"musings/","title":"Musings","text":""},{"location":"training/Kubernetes-CKAD/exam_overview/","title":"Exam Overview","text":"","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/exam_overview/#domains","title":"Domains","text":"","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/exam_overview/#application-design-build-20","title":"Application Design &amp; Build - 20%","text":"<ul> <li>Define, build and modify container images</li> <li>Choose and use the right workload resource (Deployment, DaemonSet, CronJob, etc.)</li> <li>Understand multi-container Pod design patterns (e.g. sidecar, init and others)</li> <li>Utilize persistent and ephemeral volumes</li> </ul>","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/exam_overview/#application-deployment-20","title":"Application Deployment - 20%","text":"<ul> <li>Use Kubernetes primitives to implement common deployment strategies (e.g. blue/green or canary)</li> <li>Understand Deployments and how to perform rolling updates</li> <li>Use the Helm package manager to deploy existing packages</li> <li>Kustomize</li> </ul>","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/exam_overview/#application-observability-and-maintenance-15","title":"Application Observability and Maintenance - 15%","text":"<ul> <li>Understand API deprecations</li> <li>Implement probes and health checks</li> <li>Use built-in CLI tools to monitor Kubernetes applications</li> <li>Utilize container logs</li> <li>Debugging in Kubernetes</li> </ul>","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/exam_overview/#application-environment-configuration-and-security-25","title":"Application Environment, Configuration and Security - 25%","text":"<ul> <li>Discover and use resources that extend Kubernetes (CRD, Operators)</li> <li>Understand authentication, authorization and admission control</li> <li>Understand requests, limits, quotas</li> <li>Understand ConfigMaps</li> <li>Define resource requirements</li> <li>Create &amp; consume Secrets</li> <li>Understand ServiceAccounts</li> <li>Understand Application Security (SecurityContexts, Capabilities, etc.)</li> </ul>","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/exam_overview/#services-and-networking-20","title":"Services and Networking - 20%","text":"<ul> <li>Demonstrate basic understanding of NetworkPolicies</li> <li>Provide and troubleshoot access to applications via services</li> <li>Use Ingress rules to expose applications</li> </ul>","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/intro_to_k8s_lf/","title":"Introduction to Kubernetes (LFS158x) Notes","text":"","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/intro_to_k8s_lf/#chapter-8-building-blocks","title":"Chapter 8 - Building Blocks","text":"","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/intro_to_k8s_lf/#nodes","title":"Nodes","text":"<ul> <li>Each node is managed by two k8s agents - kubelet &amp; kube-proxy as well as hosting a container runtime.</li> <li>Node identities are created and assigned during the cluster bootstrapping process. Minikube is using the default kubeadm bootstrapping tool, to initialize the control plane node during the init phase and grow the cluster by adding worker or control plane nodes with the join phase.</li> </ul>","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/intro_to_k8s_lf/#namespaces","title":"Namespaces","text":"<ul> <li><code>kube-system</code> is the default for k8s system resources.</li> <li>Create new namespaces with <code>kubectl create namespace new-namespace-name</code></li> <li>Resource quotas help users limit the overall resources consumed within Namespaces, while LimitRanges help limit the resources consumed by Containers and their enclosing objects in a Namespace. We will briefly cover quota management in a later chapter.</li> </ul>","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/intro_to_k8s_lf/#pods","title":"Pods","text":"<ul> <li> <p>A Pod is a logical collection of one or more containers, enclosing and isolating them to ensure that they:</p> <ul> <li>Are scheduled together on the same host with the Pod.</li> <li>Share the same network namespace, meaning that they share a single IP address originally assigned to the Pod.</li> <li>Have access to mount the same external storage (volumes) and other common dependencies.</li> </ul> </li> <li> <p>Pod \"healing\" is handled by controllers (Deployments, ReplicaSets, DaemonSets, Jobs, etc).</p> </li> <li>The <code>spec</code> is evaluated for scheduling and the <code>kubelet</code> of the node handles the running of the container with the container runtime.</li> </ul>","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/intro_to_k8s_lf/#labels","title":"Labels","text":"<ul> <li>Key/Value pairs attached to objects for organization and selection</li> </ul>","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/intro_to_k8s_lf/#selectors","title":"Selectors","text":"<ul> <li>Equality-Based Selectors allow filtering of objects based on Label keys and values. Matching is achieved using the =, == (equals, used interchangeably), or != (not equals) operators. For example, with <code>env==dev</code> or <code>env=dev</code> we are selecting the objects where the env Label key is set to value dev.</li> <li>Set-Based Selectors allow filtering of objects based on a set of values. We can use in, notin operators for Label values, and exist/does not exist operators for Label keys. For example, with <code>env in (dev,qa)</code> we are selecting objects where the env Label is set to either dev or qa; with <code>!app</code> we select objects with no Label key app.</li> </ul>","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/intro_to_k8s_lf/#replication-controllers-deprecated","title":"Replication Controllers (Deprecated)","text":"<p>The default recommended controller is the Deployment which configures a ReplicaSet controller to manage application Pods' lifecycle.</p>","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/intro_to_k8s_lf/#replicasets","title":"ReplicaSets","text":"<p>apiVersion: apps/v1</p> <ul> <li>A ReplicaSet is, in part, the next-generation ReplicationController, as it implements the replication and self-healing aspects of the ReplicationController. ReplicaSets support both equality- and set-based Selectors, whereas ReplicationControllers only support equality-based Selectors.</li> <li>ReplicaSets can be used independently as Pod controllers but they only offer a limited set of features. A set of complementary features are provided by Deployments, the recommended controllers for the orchestration of Pods. Deployments manage the creation, deletion, and updates of Pods. A Deployment automatically creates a ReplicaSet, which then creates a Pod. There is no need to manage ReplicaSets and Pods separately, the Deployment will manage them on our behalf.</li> </ul>","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/intro_to_k8s_lf/#deployments","title":"Deployments","text":"<p>apiVersion: apps/v1</p> <ul> <li>It allows for seamless application updates and rollbacks, known as the default RollingUpdate strategy, through rollouts and rollbacks, and it directly manages its ReplicaSets for application scaling. It also supports a disruptive, less popular update strategy, known as Recreate.</li> <li>A rolling update is triggered when we update specific properties of the Pod Template for a deployment. While planned changes such as updating the container image, container port, volumes, and mounts would trigger a new Revision, other operations that are dynamic in nature, like scaling or labeling the deployment, do not trigger a rolling update, thus do not change the Revision number.</li> </ul>","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/intro_to_k8s_lf/#daemonsets","title":"DaemonSets","text":"<ul> <li>DaemonSets are operators designed to manage node agents. They resemble ReplicaSet and Deployment operators when managing multiple Pod replicas and application updates, but the DaemonSets present a distinct feature that enforces a single Pod replica to be placed per Node, on all the Nodes. In contrast, the ReplicaSet and Deployment operators by default have no control over the scheduling and placement of multiple Pod replicas on the same Node.</li> <li>Whenever a Node is added to the cluster, a Pod from a given DaemonSet is automatically placed on it.</li> <li>Similar definition file to ReplicaSet or Deployment, but no \"replicas\" key in the spec as it's on all nodes.</li> </ul>","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/intro_to_k8s_lf/#services","title":"Services","text":"","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/intro_to_k8s_lf/#chapter-9-authentication-authorization-admission-control","title":"Chapter 9 - Authentication, Authorization, Admission Control","text":"","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/intro_to_k8s_lf/#authentication","title":"Authentication","text":"","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/intro_to_k8s_lf/#users","title":"Users","text":"<ul> <li>Normal Users: Managed outside the cluster</li> <li>Service Accounts: In-cluster processes communicating with the API server.  Authenticate with Secrets and tied to a namespace.</li> </ul>","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/intro_to_k8s_lf/#authentication-modules","title":"Authentication Modules","text":"<ul> <li>X509 Client Certificates: through passing the <code>--client-ca-file=SOMEFILE</code> option to the API server</li> <li>Static Token File: through passing the <code>--token-auth-file=SOMEFILE</code> option to the API</li> <li>Bootstrap Tokens: to bootstrap new clusters</li> <li>Service Account Tokens: use signed bearer tokens</li> <li>OpenID Connect Tokens: helps connect to OAuth2 providers like AzureAD</li> <li>Webhook Token Authentication: bearer token verification offloaded to remote service</li> <li>Authenticating Proxy: custom programming</li> </ul> <p>We can enable multiple authenticators, and the first module to successfully authenticate the request short-circuits the evaluation. To ensure successful user authentication, we should enable at least two methods: the service account tokens authenticator and one of the user authenticators.</p>","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/intro_to_k8s_lf/#authorization","title":"Authorization","text":"<p>Some of the API request attributes that are reviewed by Kubernetes include user, group, Resource, Namespace, or API group, to name a few. Next, these attributes are evaluated against policies. If the evaluation is successful, then the request is allowed, otherwise it is denied.</p> <ul> <li>Node authorization: special purpose for kubelet calls</li> <li>Attribute-Based Access Control (ABAC): combines policies with attributes. e.g.</li> </ul> <pre><code> {\n     \"apiVersion\": \"abac.authorization.kubernetes.io/v1beta1\",\n     \"kind\": \"Policy\",\n     \"spec\": {\n         \"user\": \"bob\",\n         \"namespace\": \"lfs158\",\n         \"resource\": \"pods\",\n         \"readonly\": true\n     }\n }\n</code></pre> <p>To enable ABAC mode, we start the API server with the <code>--authorization-mode=ABAC</code> option, while specifying the authorization policy with <code>--authorization-policy-file=PolicyFile.json</code>.</p> <ul> <li>Webhook: we need to start the API server with the <code>--authorization-webhook-config-file=SOME_FILENAME</code> option, where <code>SOME_FILENAME</code> is the configuration of the remote authorization service</li> <li> <p>Role-Based Access Control (RBAC): through starting the API server with the <code>--authorization-mode=RBAC</code> option</p> <ul> <li>Role: A Role grants access to resources within a specific Namespace.</li> <li>ClusterRole: A ClusterRole grants the same permissions as Role does, but its scope is cluster-wide.</li> </ul> <p>For example, this manifest defines a <code>pod-reader</code> role:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: lfs158\n  name: pod-reader\nrules:\n  - apiGroups: [\"\"] # \"\" indicates the core API group\n    resources: [\"pods\"]\n    verbs: [\"get\", \"watch\", \"list\"]\n</code></pre> <p>A <code>Role</code> can be bound to users with the <code>RoleBinding</code> object:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: pod-read-access\n  namespace: lfs158\nsubjects:\n  - kind: User\n    name: bob\n    apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> </li> </ul>","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/intro_to_k8s_lf/#demo","title":"Demo","text":"","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/intro_to_k8s_lf/#admission-control","title":"Admission Control","text":"<p>https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/</p>","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/intro_to_k8s_lf/#what-are-they","title":"What are they?","text":"<p>An admission controller is a piece of code that intercepts requests to the Kubernetes API server prior to persistence of the object, but after the request is authenticated and authorized.</p> <p>Admission controllers may be validating, mutating, or both. Mutating controllers may modify related objects to the requests they admit; validating controllers may not.</p>","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/intro_to_k8s_lf/#chapter-10-services","title":"Chapter 10 - Services","text":"<p>Kubernetes provides a higher-level abstraction (than IP addresses) called Service, which logically groups Pods and defines a policy to access them. This grouping is achieved via Labels and Selectors.  Labels and Selectors use a key-value pair format.</p> <p>Services can expose single Pods, ReplicaSets, Deployments, DaemonSets, and StatefulSets. When exposing the Pods managed by an operator, the Service's Selector may use the same label(s) as the operator.</p> <p>Example:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: frontend-svc\nspec:\n  selector:\n    app: frontend\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 5000\n</code></pre> <p>While the Service forwards traffic to Pods, we can select the targetPort on the Pod which receives the traffic. In our example, the frontend-svc Service receives requests from the user/client on port: 80 and then forwards these requests to one of the attached Pods on the targetPort: 5000. If the targetPort is not defined explicitly, then traffic will be forwarded to Pods on the port on which the Service receives traffic. It is very important to ensure that the value of the targetPort, which is 5000 in this example, matches the value of the containerPort property of the Pod spec section.</p>","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/intro_to_k8s_lf/#kube-proxy","title":"kube-proxy","text":"<p>Each cluster node runs a daemon called kube-proxy, a node agent that watches the API server on the master node for the addition, updates, and removal of Services and endpoints. kube-proxy is responsible for implementing the Service configuration on behalf of an administrator or developer, in order to enable traffic routing to an exposed application running in Pods. In the example below, for each new Service, on each node, kube-proxy configures iptables rules to capture the traffic for its ClusterIP and forwards it to one of the Service's endpoints.</p> <p>The kube-proxy node agent together with the iptables implement the load-balancing mechanism of the Service when traffic is being routed to the application Endpoints. Due to restricting characteristics of the iptables this load-balancing is random by default.</p> <p>Traffic policies allow users to instruct the kube-proxy on the context of the traffic routing. The two options are Cluster and Local:</p> <ul> <li>The Cluster option allows kube-proxy to target all ready Endpoints of the Service in the load-balancing process.</li> <li>The Local option, however, isolates the load-balancing process to only include the Endpoints of the Service located on the same node as the requester Pod.</li> </ul>","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/intro_to_k8s_lf/#service-discovery","title":"Service Discovery","text":"<p>As Services are the primary mode of communication between containerized applications managed by Kubernetes, it is helpful to be able to discover them at runtime. Kubernetes supports two methods for discovering Services:</p> <ul> <li>Environment Variables: As soon as the Pod starts on any worker node, the kubelet daemon running on that node adds a set of environment variables in the Pod for all active Services.</li> <li>DNS: Kubernetes has an add-on for DNS, which creates a DNS record for each Service and its format is my-svc.my-namespace.svc.cluster.local. Services within the same Namespace find other Services just by their names.</li> </ul>","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/intro_to_k8s_lf/#service-type","title":"Service Type","text":"<ul> <li>ClusterIP is the default ServiceType. A Service receives a Virtual IP address, known as its ClusterIP. This Virtual IP address is used for communicating with the Service and is accessible only from within the cluster.</li> <li>The NodePort ServiceType, in addition to a ClusterIP, a high-port, dynamically picked from the default range 30000-32767, is mapped to the respective Service, from all the worker nodes.  The node port directs to the ClusterIP of the service.</li> <li> <p>With the LoadBalancer ServiceType:</p> <ul> <li>NodePort and ClusterIP are automatically created, and the external load balancer will route to them.</li> <li>The Service is exposed at a static port on each worker node.</li> <li> <p>The Service is exposed externally using the underlying cloud provider's load balancer feature.</p> <p>Note</p> <p>The LoadBalancer ServiceType will only work if the underlying infrastructure supports the automatic creation of Load Balancers and have the respective support in Kubernetes, as is the case with the Google Cloud Platform and AWS. If no such feature is configured, the LoadBalancer IP address field is not populated, it remains in Pending state, but the Service will still work as a typical NodePort type Service.</p> </li> </ul> </li> <li> <p>A Service can be mapped to an ExternalIP address if it can route to one or more of the worker nodes.  ExternalIPs are not managed by Kubernetes. The cluster administrator has to configure the routing which maps the ExternalIP address to one of the nodes.</p> </li> <li>ExternalName is a special ServiceType that has no Selectors and does not define any endpoints. When accessed within the cluster, it returns a CNAME record of an externally configured Service.  The primary use case of this ServiceType is to make externally configured Services like my-database.example.com available to applications inside the cluster.</li> </ul> <p>A Service resource can expose multiple ports at the same time if required. Its configuration is flexible enough to allow for multiple groupings of ports to be defined in the manifest.</p>","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/training_materials/","title":"CKAD Training Resources","text":"","tags":["ckad"]},{"location":"training/Kubernetes-CKAD/training_materials/#courses","title":"Courses","text":"<ul> <li>Introduction to Kubernetes (LFS158x)</li> <li> <p>Udemy - Kubernetes Certified Application Developer (CKAD) with Tests</p> <p>Initial thoughts:</p> <ul> <li>There is repetitive material here and one can tell that it was created a few versions prior and being reused.</li> <li>The KodeKloud \"tests\" are quite simple, but allow one to use the k8s environment to expand the knowledge and \"play\".</li> </ul> </li> </ul>","tags":["ckad"]},{"location":"workstation_configuration/local_kubernetes/","title":"Local Kubernetes Environment - Kind","text":"","tags":["kind","k8s"]},{"location":"workstation_configuration/local_kubernetes/#prerequisites-for-a-local-cluster-with-kind","title":"Prerequisites for a Local Cluster with Kind","text":"","tags":["kind","k8s"]},{"location":"workstation_configuration/local_kubernetes/#install-docker-engine-for-the-container-runtime","title":"Install Docker Engine for the Container Runtime","text":"<p>I ran into an Ubuntu packaging bug (also listed on Github) so recommend that the source Docker repo is used.  </p> <p>Note</p> <p>The following steps are excerpted from https://docs.docker.com/engine/install/ubuntu/</p> <ol> <li> <p>Remove the Ubuntu packages:</p> <pre><code>for pkg in docker.io docker-doc docker-compose podman-docker containerd runc; do sudo apt-get remove $pkg; done\n</code></pre> </li> <li> <p>Set up HTTPS transport, add Docker's GPG key, set up the Docker <code>apt</code> repository:</p> <pre><code>sudo apt-get update\nsudo apt-get install ca-certificates curl gnupg\n</code></pre> <pre><code>sudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\n</code></pre> <pre><code>echo \\\n\"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\n\"$(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\")\" stable\" | \\\nsudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n</code></pre> </li> <li> <p>Install Docker Engine</p> <pre><code>sudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> </li> <li> <p>Set up to use it as a non-root user by adding the user to the <code>docker</code> group.</p> </li> <li>Make sure the <code>docker</code> and <code>containerd</code> services are running and enabled on boot.</li> <li>Test installation with <code>docker run hello-world</code></li> </ol>","tags":["kind","k8s"]},{"location":"workstation_configuration/local_kubernetes/#install-kubectl","title":"Install kubectl","text":"<p>Note</p> <p>The following steps are excerpted from https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#install-using-native-package-management</p> <ol> <li> <p>Set up HTTPS transport, add Docker's GPG key, set up the Docker <code>apt</code> repository:</p> <pre><code>sudo apt-get update\n# apt-transport-https may be a dummy package; if so, you can skip that package\nsudo apt-get install -y apt-transport-https ca-certificates curl\n</code></pre> <pre><code>curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n</code></pre> <pre><code># This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list\n</code></pre> </li> <li> <p>Update <code>apt</code> package index, then install kubectl:</p> <pre><code>sudo apt-get update\nsudo apt-get install -y kubectl\n</code></pre> </li> </ol>","tags":["kind","k8s"]},{"location":"workstation_configuration/local_kubernetes/#install-kind","title":"Install Kind","text":"<p>For Linux, <code>kind</code> is best installed through Release Binaries on Github</p> <p>The latest version can be installed through:</p> <pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64\n# For ARM64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-arm64\nchmod +x ./kind\nsudo mv ./kind /usr/local/bin/kind\n</code></pre> <p>or downloaded from the release page, checked against the sha256sum file, made executable (<code>chmod +x ./kind</code>) and put in the path (<code>sudo mv ./kind /usr/local/bin/kind</code>)</p>","tags":["kind","k8s"]},{"location":"workstation_configuration/local_kubernetes/#use-kind-to-create-your-k8-cluster","title":"Use Kind to create your K8 cluster","text":"<p>Run the below:</p> <pre><code>kind create cluster\n</code></pre> <p>To see your cluster information run:</p> <pre><code>kubectl cluster-info --context kind-kind\n</code></pre>","tags":["kind","k8s"]},{"location":"workstation_configuration/local_kubernetes/#use-kind-to-create-a-k8-cluster-with-multiple-nodes","title":"Use Kind to create a K8 cluster with multiple nodes","text":"<p>Save the below to a file named <code>kind.config</code></p> <pre><code># this config file contains all config fields with comments\n# NOTE: this is not a particularly useful config file\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\n# patch the generated kubeadm config with some extra settings\nkubeadmConfigPatches:\n- |\n  apiVersion: kubelet.config.k8s.io/v1beta1\n  kind: KubeletConfiguration\n  evictionHard:\n    nodefs.available: \"0%\"\n# patch it further using a JSON 6902 patch\nkubeadmConfigPatchesJSON6902:\n- group: kubeadm.k8s.io\n  version: v1beta3\n  kind: ClusterConfiguration\n  patch: |\n    - op: add\n      path: /apiServer/certSANs/-\n      value: my-hostname\n# 1 control plane node and 3 workers\nnodes:\n# the control plane node config\n- role: control-plane\n# the three workers\n- role: worker\n- role: worker\n- role: worker\n</code></pre> <p>Run the below referencing your <code>kind.config</code></p> <pre><code>kind create cluster --config '.\\Documents\\Kind\\kind.config'\n</code></pre> <p>Now when you run get nodes you can see a control-plane and 3 worker nodes</p> <pre><code>kubectl get nodes --context kind-kind\n</code></pre>","tags":["kind","k8s"]},{"location":"workstation_configuration/set_up_mkdocs/","title":"Documentation with mkdocs","text":"","tags":["mkdocs"]},{"location":"workstation_configuration/set_up_mkdocs/#local-development","title":"Local Development","text":"<p>A local environment should be configured to enable linting and building <code>mkdocs</code><sup>1</sup> locally. This enables testing changes locally before committing to a remote branch.</p>","tags":["mkdocs"]},{"location":"workstation_configuration/set_up_mkdocs/#prerequisites","title":"Prerequisites","text":"","tags":["mkdocs"]},{"location":"workstation_configuration/set_up_mkdocs/#python","title":"Python","text":"<ol> <li>This is currently using Python v3.11.   A working <code>pyenv</code> installation is a good way to set this up.<sup>2</sup></li> <li>A working installation of Poetry for Python dependency management.<sup>3</sup></li> </ol>","tags":["mkdocs"]},{"location":"workstation_configuration/set_up_mkdocs/#clone-repository","title":"Clone Repository","text":"<ol> <li> <p>Clone the repository to your local machine by running:</p> <pre><code>git clone **REPO_URL**\n</code></pre> </li> </ol>","tags":["mkdocs"]},{"location":"workstation_configuration/set_up_mkdocs/#local-environment-workflow-overview","title":"Local Environment Workflow Overview","text":"<ol> <li>Run your Terminal (or IDE) of choice.</li> <li> <p>Navigate to REPO NAME repository to the <code>docs/</code> sub-folder.</p> <p>Note</p> <p>There might be two levels of \"docs\" folders in the repo.  The first level is the base of the documentation area where <code>mkdocs</code> is run and the mkdocs.yml file lives.  The second level docs/ folder under this is where the actual documentation files live that are used to generate the static pages for the documentation site.</p> </li> <li> <p>Run <code>poetry install</code> or <code>make install</code> to install the required Python packages in the poetry environment while located in the directory where the pyproject.toml file is.</p> </li> <li> <p>(Optional) Run <code>poetry shell</code> to activate the poetry virtual environment.</p> <p>Tip</p> <p>There are a few options to get the tasks done. 1. Run <code>poetry shell</code> to activate the virtual environment shell and just run the commands in that virtual environment shell. 2. Prefix the commands with <code>poetry run</code> to run the command in the virtual environment without having to run <code>poetry shell</code> first. 3. A <code>Makefile</code> is provided with command shortcuts if this method is preferred.</p> <p>The following workflow steps will include the commands for options 1. and 3.  If you chose to use neither of those, then prefix each other command with <code>poetry run</code></p> </li> <li> <p>Create and checkout a new branch.</p> </li> <li>Edit or create any documentation as needed in the second level <code>docs</code> folder.</li> <li> <p>Run <code>pymarkdownlnt -c .pymarkdown-linting-cfg.json scan --recurse docs</code> (referencing the correct path to the linting configuration file) or <code>make lint</code> to lint the markdown, and make changes where necessary.</p> <p>Info</p> <p>Details for Markdown rules can be found in the documentation. If rules need to be ignored, update the linting configuration file.</p> </li> <li> <p>Run <code>mkdocs serve</code> or <code>make serve</code> to run the mkdocs site locally and confirm the changes.</p> </li> <li>Push your changes to your branch on Github.</li> <li>Create a Pull Request (PR) for someone to review your changes.</li> <li> <p>After PR review and approval, the changes can be merged into the main or master branch.</p> <p>Todo</p> <p>Set up Github Actions for the following deployment step, when available.</p> </li> <li> <p>After the changes are merged, update your local repo <code>main</code> or <code>master</code> branch (<code>git pull</code> after checking out the correct branch). Then, from your updated <code>main</code> or <code>master</code> branch, run <code>mkdocs gh-deploy</code> or <code>make deploy</code> to push the new site to the <code>gh-pages</code> branch and make the changes to the documentation site live.</p> </li> </ol>","tags":["mkdocs"]},{"location":"workstation_configuration/set_up_mkdocs/#issues","title":"Issues","text":"","tags":["mkdocs"]},{"location":"workstation_configuration/set_up_mkdocs/#create-an-issue","title":"Create an issue","text":"<p>If you want to create, delete or update anything related to this repo, create an issue that aligns with the below:</p> <ul> <li>Add additional content to an existing category</li> <li>Creation of new category</li> <li>Spelling and grammar</li> <li>Linting for markdown</li> </ul>","tags":["mkdocs"]},{"location":"workstation_configuration/set_up_mkdocs/#solve-an-issue","title":"Solve an issue","text":"<p>Scan through our existing issues to find one that needs addressing. If you find an issue to work on, work on the fix locally in a branch, check it locally, and then create a Pull Request (PR) with the fix.</p>","tags":["mkdocs"]},{"location":"workstation_configuration/set_up_mkdocs/#branches","title":"Branches","text":"<p>Try to use one of the following when creating branches for the documentation:</p> <pre><code>create/documentation-[DOC-NAME]\n</code></pre> <pre><code>update/documentation-[DOC-NAME]\n</code></pre> <pre><code>delete/documentation-[DOC-NAME]\n</code></pre>","tags":["mkdocs"]},{"location":"workstation_configuration/set_up_mkdocs/#making-changes","title":"Making Changes","text":"","tags":["mkdocs"]},{"location":"workstation_configuration/set_up_mkdocs/#make-changes-in-the-ui","title":"Make Changes in the UI","text":"<p>Click on the edit icon at the top right of any of the pages to make small changes such as a typographic errors, grammar fixes, or a broken link. This takes you to the .md file where you can make your changes and create a pull request for a review.</p>","tags":["mkdocs"]},{"location":"workstation_configuration/set_up_mkdocs/#make-changes-locally","title":"Make Changes Locally","text":"","tags":["mkdocs"]},{"location":"workstation_configuration/set_up_mkdocs/#update-existing-content","title":"Update Existing Content","text":"<ol> <li>Make changes required to the <code>index.md</code> of the existing category, eg: technical-solutions</li> <li>Run linting command from local development environment to ensure you're following the Markdown rules, and resolve any issues</li> <li>Run mkdocs server locally to ensure the site will successfully build without errors via your local development environment</li> <li>Open <code>http://127.0.0.1:8000/</code> via browser and validate changes have applied as expected</li> </ol>","tags":["mkdocs"]},{"location":"workstation_configuration/set_up_mkdocs/#create-new-category","title":"Create New Category","text":"<ol> <li>Create a category folder</li> <li>Create <code>index.md</code> within the folder created</li> <li>Run linting command from your local development environment to ensure you're following the Markdown rules, and resolve any issues</li> <li>Run mkdocs server locally to ensure the site will successfully build without errors via your local development environment</li> <li>Open <code>http://127.0.0.1:8000/</code> via browser and validate changes have applied as expected</li> </ol>","tags":["mkdocs"]},{"location":"workstation_configuration/set_up_mkdocs/#commits","title":"Commits","text":"","tags":["mkdocs"]},{"location":"workstation_configuration/set_up_mkdocs/#create-commit","title":"Create Commit","text":"<ul> <li>Review the content for accuracy before creating your commit</li> <li>Check your changes for grammar and spelling before creating your commit</li> <li>Keep commit messages short (&lt;80 characters)</li> <li>Make an individual commit for each category</li> <li>Squash multiple commits</li> </ul>","tags":["mkdocs"]},{"location":"workstation_configuration/set_up_mkdocs/#pull-requests","title":"Pull Requests","text":"","tags":["mkdocs"]},{"location":"workstation_configuration/set_up_mkdocs/#create-pull-request","title":"Create Pull Request","text":"<p>When you're finished with the changes, create a pull request (PR).</p> <ul> <li>Use imperative tense (e.g. \"add\" instead of \"added\" or \"adding\") in the PR title.</li> <li>Include application names, categories and a link to the project page in the description.</li> <li>Don't forget to link your PR to an issue<sup>4</sup> if you are solving one.</li> <li>We may ask for changes to be made before a PR can be merged, either using suggested changes<sup>5</sup> or pull request comments. You can apply suggested changes directly through the UI. You can make any other changes in your fork, then commit them to your branch.</li> <li>As you update your PR and apply changes, mark each conversation as resolved.<sup>6</sup></li> <li>If you run into any merge issues, checkout the git tutorial on \"merge conflicts\" and other issues<sup>7</sup>.</li> </ul>","tags":["mkdocs"]},{"location":"workstation_configuration/set_up_mkdocs/#merge-your-pr","title":"Merge your PR","text":"<p>Congratulations \ud83c\udf89\ud83c\udf89 the team thanks you \u2728.</p> <p>Once your PR is merged, your contributions can be pushed to the gh-pages branch and will be visible on Documentation Site.</p>","tags":["mkdocs"]},{"location":"workstation_configuration/set_up_mkdocs/#relevant-documentation","title":"Relevant Documentation","text":"<ol> <li> <p>https://www.mkdocs.org/ \u21a9</p> </li> <li> <p>https://github.com/pyenv/pyenv#simple-python-version-management-pyenv \u21a9</p> </li> <li> <p>https://python-poetry.org/docs/#installation \u21a9</p> </li> <li> <p>https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue \u21a9</p> </li> <li> <p>https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/reviewing-changes-in-pull-requests/incorporating-feedback-in-your-pull-request \u21a9</p> </li> <li> <p>https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/reviewing-changes-in-pull-requests/commenting-on-a-pull-request#resolving-conversations \u21a9</p> </li> <li> <p>https://github.com/skills/resolve-merge-conflicts \u21a9</p> </li> </ol>","tags":["mkdocs"]}]}